{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Take pictues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ValidatedGraphConfig Initialization failed.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:155) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()  \u001b[38;5;66;03m# Close all OpenCV windows\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Test the function with an image path\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43mdraw_landmarks_on_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaptured_image.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m, in \u001b[0;36mdraw_landmarks_on_camera\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_landmarks_on_camera\u001b[39m(img_path):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize MediaPipe Face Mesh\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n\u001b[0;32m----> 7\u001b[0m     face_mesh \u001b[38;5;241m=\u001b[39m \u001b[43mmp_face_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceMesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Read the image\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/python/solutions/face_mesh.py:95\u001b[0m, in \u001b[0;36mFaceMesh.__init__\u001b[0;34m(self, static_image_mode, max_num_faces, refine_landmarks, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m              static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m              max_num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     73\u001b[0m              refine_landmarks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m              min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     75\u001b[0m              min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a MediaPipe Face Mesh object.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m      https://solutions.mediapipe.dev/face_mesh#min_tracking_confidence.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbinary_graph_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_BINARYPB_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m      \u001b[49m\u001b[43mside_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_faces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwith_attention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_landmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_prev_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcalculator_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacedetectionshortrangecpu__facedetectionshortrange__facedetection__TensorsToDetectionsCalculator.min_score_thresh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacelandmarkcpu__ThresholdingCalculator.threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_tracking_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_face_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/python/solution_base.py:248\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_options:\n\u001b[1;32m    245\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_extension(canonical_graph_config_proto\u001b[38;5;241m.\u001b[39mgraph_options,\n\u001b[1;32m    246\u001b[0m                       graph_options)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcalculator_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCalculatorGraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanonical_graph_config_proto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ValidatedGraphConfig Initialization failed.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:155) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def draw_landmarks_on_camera(img_path):\n",
    "    # Initialize MediaPipe Face Mesh\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to load image at path '{img_path}'\")\n",
    "        return\n",
    "\n",
    "    # Resize for faster processing (optional)\n",
    "    img = cv2.resize(img, (640, 480))\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    # Draw landmarks if detected\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = img.shape\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x = int(landmark.x * w)  # Convert normalized x to pixel\n",
    "                y = int(landmark.y * h)  # Convert normalized y to pixel\n",
    "                cv2.circle(img, (x, y), 2, (0, 255, 0), -1)  # Draw dot\n",
    "    else:\n",
    "        print(\"No face detected.\")\n",
    "\n",
    "    # Show the processed image with landmarks\n",
    "    cv2.imshow(\"3D Facial Landmarks\", img)\n",
    "    cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "    cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "\n",
    "# Test the function with an image path\n",
    "draw_landmarks_on_camera(\"captured_image.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ValidatedGraphConfig Initialization failed.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:155) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 81\u001b[0m\n\u001b[1;32m     77\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     78\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m---> 81\u001b[0m \u001b[43mcapture_image_with_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscanned_face.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mcapture_image_with_landmarks\u001b[0;34m(output_path, delay)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize MediaPipe Face Mesh\u001b[39;00m\n\u001b[1;32m     16\u001b[0m mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n\u001b[0;32m---> 17\u001b[0m face_mesh \u001b[38;5;241m=\u001b[39m \u001b[43mmp_face_mesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceMesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_tracking_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n\u001b[1;32m     20\u001b[0m mp_drawing_styles \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_styles\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/python/solutions/face_mesh.py:95\u001b[0m, in \u001b[0;36mFaceMesh.__init__\u001b[0;34m(self, static_image_mode, max_num_faces, refine_landmarks, min_detection_confidence, min_tracking_confidence)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m              static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m              max_num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     73\u001b[0m              refine_landmarks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m              min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     75\u001b[0m              min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a MediaPipe Face Mesh object.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m      https://solutions.mediapipe.dev/face_mesh#min_tracking_confidence.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbinary_graph_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_BINARYPB_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m      \u001b[49m\u001b[43mside_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_faces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwith_attention\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefine_landmarks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_prev_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstatic_image_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcalculator_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacedetectionshortrangecpu__facedetectionshortrange__facedetection__TensorsToDetectionsCalculator.min_score_thresh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacelandmarkcpu__ThresholdingCalculator.threshold\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmin_tracking_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m      \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmulti_face_landmarks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/python/solution_base.py:248\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_options:\n\u001b[1;32m    245\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_extension(canonical_graph_config_proto\u001b[38;5;241m.\u001b[39mgraph_options,\n\u001b[1;32m    246\u001b[0m                       graph_options)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcalculator_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCalculatorGraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanonical_graph_config_proto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ValidatedGraphConfig Initialization failed.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nConstantSidePacketCalculator: ; RET_CHECK failure (mediapipe/calculators/core/constant_side_packet_calculator.cc:66) (cc->OutputSidePackets().NumEntries(kPacketTag))==(options.packet_size())Number of output side packets has to be same as number of packets configured in options.\nImageToTensorCalculator: ; RET_CHECK failure (mediapipe/calculators/tensor/image_to_tensor_calculator.cc:155) ValidateOptionOutputDims(options) returned INTERNAL: ; RET_CHECK failure (./mediapipe/calculators/tensor/image_to_tensor_utils.h:136) options.has_output_tensor_float_range() || options.has_output_tensor_int_range() || options.has_output_tensor_uint_range()Output tensor range is required. \nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions.\nSplitTensorVectorCalculator: The number of output streams should match the number of ranges specified in the CalculatorOptions."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def capture_image_with_landmarks(output_path=\"captured_image_with_landmarks.jpg\", delay=5):\n",
    "    \"\"\"\n",
    "    Detects facial landmarks using MediaPipe, applies a wavy effect to show scanning, \n",
    "    and captures a picture with the landmarks drawn.\n",
    "    \n",
    "    Args:\n",
    "        output_path: File path where the captured image will be saved.\n",
    "        delay: Time in seconds before capturing the image.\n",
    "    \"\"\"\n",
    "    # Initialize MediaPipe Face Mesh\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                       min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open the default camera\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame from camera.\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB for MediaPipe processing\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        # Draw landmarks with a wave effect if detected\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                for i, landmark in enumerate(face_landmarks.landmark):\n",
    "                    h, w, _ = frame.shape\n",
    "                    x = int(landmark.x * w)\n",
    "                    y = int(landmark.y * h)\n",
    "\n",
    "                    # Wave effect: Alter the color or size over time\n",
    "                    wave_intensity = int(127 + 127 * np.sin(2 * np.pi * i / len(face_landmarks.landmark) + time.time()))\n",
    "                    color = (0, wave_intensity, 255 - wave_intensity)\n",
    "                    cv2.circle(frame, (x, y), 2, color, -1)\n",
    "\n",
    "                # Draw the full face mesh for context\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    face_landmarks,\n",
    "                    mp_face_mesh.FACE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1),\n",
    "                )\n",
    "\n",
    "        # Show the live camera feed with wave effect\n",
    "        remaining_time = int(delay - (time.time() - start_time))\n",
    "        if remaining_time > 0:\n",
    "            cv2.putText(frame, f\"Capturing in {remaining_time}s\", (50, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # Save the frame with landmarks\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            print(f\"Image captured and saved as {output_path}\")\n",
    "            break\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Facial Landmarks with Wave Effect\", frame)\n",
    "\n",
    "        # Exit if user presses ESC\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            print(\"Cancelled by user.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "capture_image_with_landmarks(output_path=\"scanned_face.jpg\", delay=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown We implemented some functions to visualize the face landmark detection results. <br/> Run the following cell to activate the functions.\n",
    "\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  face_landmarks_list = detection_result.face_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected faces to visualize.\n",
    "  for idx in range(len(face_landmarks_list)):\n",
    "    face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "    # Draw the face landmarks.\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "    ])\n",
    "\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_tesselation_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "        image=annotated_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "          landmark_drawing_spec=None,\n",
    "          connection_drawing_spec=mp.solutions.drawing_styles\n",
    "          .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "  return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "  # Extract the face blendshapes category names and scores.\n",
    "  face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "  face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "  # The blendshapes are ordered in decreasing score value.\n",
    "  face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(12, 12))\n",
    "  bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores, label=[str(x) for x in face_blendshapes_ranks])\n",
    "  ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "  ax.invert_yaxis()\n",
    "\n",
    "  # Label each bar with values\n",
    "  for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "    plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "  ax.set_xlabel('Score')\n",
    "  ax.set_title(\"Face Blendshapes\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"captured_image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FieldDescriptor' object has no attribute '_default_constructor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_landmarker_v2_with_blendshapes.task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m options \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mFaceLandmarkerOptions(base_options\u001b[38;5;241m=\u001b[39mbase_options,\n\u001b[1;32m      9\u001b[0m                                        output_face_blendshapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                                        output_facial_transformation_matrixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                                        num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# STEP 3: Load the input image.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m image \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mcreate_from_file(img)\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/tasks/python/vision/face_landmarker.py:3105\u001b[0m, in \u001b[0;36mFaceLandmarker.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m   3091\u001b[0m   output_streams\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   3092\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_FACE_GEOMETRY_TAG, _FACE_GEOMETRY_STREAM_NAME])\n\u001b[1;32m   3093\u001b[0m   )\n\u001b[1;32m   3095\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[1;32m   3096\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m   3097\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   3103\u001b[0m )\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m-> 3105\u001b[0m     \u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3106\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[1;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3109\u001b[0m     options\u001b[38;5;241m.\u001b[39mrunning_mode,\n\u001b[1;32m   3110\u001b[0m     packets_callback \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mresult_callback \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3111\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/mediapipe/tasks/python/core/task_info.py:97\u001b[0m, in \u001b[0;36mTaskInfo.generate_graph_config\u001b[0;34m(self, enable_flow_limiting)\u001b[0m\n\u001b[1;32m     93\u001b[0m   task_subgraph_options \u001b[38;5;241m=\u001b[39m calculator_options_pb2\u001b[38;5;241m.\u001b[39mCalculatorOptions()\n\u001b[1;32m     94\u001b[0m   task_subgraph_options\u001b[38;5;241m.\u001b[39mExtensions[task_options_proto\u001b[38;5;241m.\u001b[39mext]\u001b[38;5;241m.\u001b[39mCopyFrom(\n\u001b[1;32m     95\u001b[0m       task_options_proto\n\u001b[1;32m     96\u001b[0m   )\n\u001b[0;32m---> 97\u001b[0m   \u001b[43mnode_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCopyFrom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_subgraph_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m   \u001b[38;5;66;03m# Use the Any type for task_subgraph_options (proto3)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m   task_subgraph_options \u001b[38;5;241m=\u001b[39m any_pb2\u001b[38;5;241m.\u001b[39mAny()\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/google/protobuf/message.py:129\u001b[0m, in \u001b[0;36mMessage.CopyFrom\u001b[0;34m(self, other_msg)\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mClear()\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMergeFrom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother_msg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College/Capstone_2024/venv/lib/python3.12/site-packages/google/protobuf/internal/python_message.py:1334\u001b[0m, in \u001b[0;36m_AddMergeFromMethod.<locals>.MergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1331\u001b[0m field_value \u001b[38;5;241m=\u001b[39m fields\u001b[38;5;241m.\u001b[39mget(field)\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1333\u001b[0m   \u001b[38;5;66;03m# Construct a new object to represent this field.\u001b[39;00m\n\u001b[0;32m-> 1334\u001b[0m   field_value \u001b[38;5;241m=\u001b[39m \u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_constructor\u001b[49m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1335\u001b[0m   fields[field] \u001b[38;5;241m=\u001b[39m field_value\n\u001b[1;32m   1336\u001b[0m field_value\u001b[38;5;241m.\u001b[39mMergeFrom(value)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FieldDescriptor' object has no attribute '_default_constructor'"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(img)\n",
    "\n",
    "# STEP 4: Detect face landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv2.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
